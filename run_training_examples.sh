#!/bin/bash
# Example training commands for train_gpt.py
# Demonstrates different model sizes and configurations for 8xH100 setup

export TOKENIZERS_PARALLELISM=false  # Avoid tokenizer warnings

# Example 1: 7B model with BF16 and all optimizations (recommended for H100)
echo "Example 1: Training 7B model with BF16 precision and full optimizations"
echo "Command:"
echo "torchrun --nproc_per_node=8 train_gpt.py \\"
echo "    --model_size 7B \\"
echo "    --batch_size 4 \\"
echo "    --precision bf16 \\"
echo "    --gradient_accumulation_steps 4 \\"
echo "    --max_steps 1000 \\"
echo "    --use_flash_attn \\"
echo "    --use_fused_optimizer \\"
echo "    --output_dir ./checkpoints/7b_bf16_optimized"
echo ""

# Example 2: 3B model with higher batch size
echo "Example 2: Training 3B model with higher batch size"
echo "Command:"
echo "torchrun --nproc_per_node=8 train_gpt.py \\"
echo "    --model_size 3B \\"
echo "    --batch_size 8 \\"
echo "    --precision bf16 \\"
echo "    --gradient_accumulation_steps 2 \\"
echo "    --max_steps 2000 \\"
echo "    --learning_rate 3e-4 \\"
echo "    --output_dir ./checkpoints/3b_bf16"
echo ""

# Example 3: 1.3B model for testing
echo "Example 3: Training 1.3B model for quick testing"
echo "Command:"
echo "torchrun --nproc_per_node=8 train_gpt.py \\"
echo "    --model_size 1.3B \\"
echo "    --batch_size 16 \\"
echo "    --precision bf16 \\"
echo "    --gradient_accumulation_steps 1 \\"
echo "    --max_steps 500 \\"
echo "    --output_dir ./checkpoints/1.3b_test"
echo ""

# Example 4: Memory testing - just check feasibility without training
echo "Example 4: Memory feasibility check (8B model)"
echo "Command:"
echo "python train_gpt.py \\"
echo "    --model_size 8B \\"
echo "    --batch_size 2 \\"
echo "    --precision bf16 \\"
echo "    --max_steps 1  # Just one step to check memory"
echo ""

# Example 5: Full-scale 8B training with all optimizations
echo "Example 5: Full-scale 8B model training with high-performance optimizations"
echo "Command:"
echo "torchrun --nproc_per_node=8 train_gpt.py \\"
echo "    --model_size 8B \\"
echo "    --batch_size 2 \\"
echo "    --precision bf16 \\"
echo "    --gradient_accumulation_steps 8 \\"
echo "    --sequence_length 2048 \\"
echo "    --learning_rate 1.5e-4 \\"
echo "    --warmup_steps 2000 \\"
echo "    --max_steps 10000 \\"
echo "    --save_steps 1000 \\"
echo "    --log_steps 50 \\"
echo "    --use_flash_attn \\"
echo "    --use_fused_optimizer \\"
echo "    --tokenizer_num_workers 8 \\"
echo "    --prefetch_factor 6 \\"
echo "    --output_dir ./checkpoints/8b_full_scale_optimized \\"
echo "    --monitor_memory \\"
echo "    --log_timing"
echo ""

echo "Notes:"
echo "- Adjust batch_size and gradient_accumulation_steps based on your memory"
echo "- Use BF16 precision for H100 GPUs (better than FP16)"
echo "- Enable --use_flash_attn for better memory efficiency and speed"
echo "- Enable --use_fused_optimizer for faster optimizer steps"
echo "- Increase --tokenizer_num_workers and --prefetch_factor for better data throughput"
echo "- Monitor memory usage with --monitor_memory flag"
echo "- Total effective batch size = batch_size × num_gpus × gradient_accumulation_steps"
echo "- For power monitoring, use additional tools like nvidia-smi or nsight-systems"
echo ""
echo "Memory guidelines for H100 80GB:"
echo "- 1.3B: batch_size=16, no gradient accumulation needed"
echo "- 3B: batch_size=8, gradient_accumulation_steps=2-4"
echo "- 7B: batch_size=4, gradient_accumulation_steps=4-8"  
echo "- 8B: batch_size=2, gradient_accumulation_steps=8-16" 